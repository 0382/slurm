<!--#include virtual="header.txt"-->

<h1>IBM Parallel Environment User and Administrator Guide</h1>

<h2>Overview</h2>

<p>This document describes the unique features of SLURM on the
IBM computers with the
<a href="http://www-03.ibm.com/systems/software/parallel/">Parallel Environment (PE)</a>
software. You should be familiar with the SLURM's mode of operation on Linux
clusters before studying the relatively few differences in operation on systems
with PE, which are described in this document.</p>

<h2>User Tools</h2>

<p>The normal set of SLURM user tools: srun, scancel, sinfo, squeue, scontrol,
etc. provide all of the expected services.
The only SLURM command not supported is sattach.
Job steps are launched using the
srun command, which translates its options and invokes IBM's poe command. The
poe command actually launches the tasks. The poe command may also be invoked
directly if desired. The actual task launch process is as follows:</p>
<ol>
<li>Invoke srun command with desired options.</li>
<li>The srun command creates a job allocation (if necessary).</li>
<li>The srun command translates its options and invokes the poe command.</li>
<li>The poe command loads a SLURM library that provides various resource
management functionality.</li>
<li>The poe command, through the SLURM library, launches a process named
"pmdv12" on the appropriate compute nodes. Note that the "v12" on the end of
the process name represents the version number of the "pmd" process and is
subject to change.</li>
<li>The poe command interacts with the pmdv12 process to launch the application
tasks, handle their I/O, etc. Since the task launch procedure occurs outside of
SLURM's control, none of the normal task-level SLURM support is available.</li>
<li>The poe command, through the SLURM library, notes the completion of the
job step.</li>
</ol>

<h3>Network Options</h3>

<p>Each job step can specify it's desired network options.
For example, one job step may use IP mode communications and the next use
User Space (US) mode communications.
Network specifications may be specified using srun's --network option or the
SLURM_NETWORK environment variable. Supported network options include:</p>

<ul>
<li>Network protocol</li>
  <ul>
  <li><b>ip</b> Internet protocol, version 4</li>
  <li><b>ipv4</b> Internet protocol, version 4 (default)</li>
  <li><b>ipv6</b> Internet protocol, version 6</li>
  <li><b>us</b> User Space protocol, may be combined with ibv4 or ipv6</li>
  </ul>
<li>Programming interface</li>
  <ul>
  <li><b>lapi</b> Low-level Application Programming Interface</li>
  <li><b>mpi</b> Message Passing Interface (default)</li>
  <li><b>pami</b> Parallel Active Message Interface</li>
  <li><b>upc</b> Unified Parallel C Interface</li>
  </ul>
<li>Other options</li>
  <ul>
  <li><b>bulk_xfer [=<i>resources</i>]</b>
  Enable bulk transfer of data using Remote Direct-Memory Access (RDMA).
  The optional <i>resources</i> specification is a numeric value which can have
  a suffix of "k", "K", "m", "M", "g" or "G" for kilobytes, megabytes or
  gigabytes.
  <b>NOTE:</b> The <i>resources</i> specification is not supported by the
  underlying IBM infrastructure as of Parallel Environment version 2.2 and no
  value should be specified at this time.</li>
  <li><b>cau=<i>count</i></b>
  Specify the count of Collective Acceleration Units (CAU) required per
  programming interface.
  Default value is zero.
  Applies only to IBM Power7-IH processors.
  <li><b>devname=<i>name</i></b>
  Specify the name of an network adapter to use.
  For example: "eth0" or "mlx4_0".</li>
  <li><b>devtype=<i>type</i></b>
  Specify the device type to use for communications.
  The supported values of <i>type</i> are:
  "IB" (InfiniBand), "HFI" (P7 Host Fabric Interface),
  "IPONLY" (IP-Only interfaces), "HPCE" (HPC Ethernet), and
  "KMUX" (Kernel Emulation of HPCE).
  The devices allocated to a job must all be of the same type.
  The default value depends upon depends upon what hardware is available and in
  order of preferences is IPONLY (which is not considered in User Space mode),
  HFI, IB, HPCE, and KMUX.</li>
  <li><b>immed=<i>count</i></b>
  Specify the count of immediate send slots per adapter window.
  Default value is zero.
  Applies only to IBM Power7-IH processors.
  <li><b>instances=<i>count</i></b>
  Specify number of network connections for each task on each network connection.
  The default instance count is 1.</li>
  <li><b>sn_all</b> Use all available switch adapters (default).
  This option can not be combined with sn_single.</li>
  <li><b>sn_single</b> Use only one switch adapters.
  This option can not be combined with sn_all.
  If multiple adapters of different types exist, the devname and/or
  devtype option can also be used to select one of them.</li>
  </ul>
</ul>

<p>Examples of network option use:
<br><br>
<b>--network=sn_all,mpi</b><br>
Allocate one switch window per task on each node and every network supporting
MPI.
<br><br>
<b>--network=sn_all,mpi,bulk_xfer,us</b><br>
Allocate one switch window per task on each node and every network supporting
MPI and user space communications. Reserve resources for RDMA.
<br><br>
<b>--network=sn_all,instances=3,mpi</b><br>
Allocate three switch window per task on each node and every network supporting
MPI.
<br><br>
<b>--network=sn_all,mpi,pami</b><br>
Allocate one switch window per task on each node and every network supporting
MPI and a second window supporting PAMI.
<br><br>
<b>--network=devtype=ib,instances=2,lapi,mpi</b><br>
On every Infiniband network connection, allocate two switch windows each for 
both lapi and mpi interfaces. If each node has one Infinband network connection,
this would result in four switch windows per task.
</p>

<p><b>NOTE:</b> Switch resources on a node are shared between all job steps on
that node. If a job step can not be initiated due to insufficient switch
resources being available, that job step will periodically retry getting
resources for the lifetime of the job unless srun's --immediate option is
used.</p>

<h3>Debugging</h3>

<p>Most debuggers require detailed information about launched tasks such as
host name, process ID, etc. Since that information is only available from
poe (which launches those tasks), the srun command wrapper can not be used
for most debugging purposes. You or the debugging tool must invoke the poe
command directly. In order to facilitate use of poe, srun's 
<br><b>--launch-cmd</b> option may be used with the options normally used.
srun will then print the equivalent poe command line, which
can subsequently be used with the debugger.</p>

<h3>Checkpoint</h3>

<p><b>FIXME: We need to validate all of this</b>
SLURM supports checkpoint via poe.
In order to enable checkpoint, the shell executing the poe command must
itself be initiated with the environment variable <b>CHECKPOINT=yes</b>.
One file is written for each node on which the job is executing, plus
another for the script executing poe.a
By default, the checkpoint files will be written to the current working
directory of the job.
Names and locations of these files can be controlled using the
environment variables <b>MP_CKPTFILE</b> and <b>MP_CKPTDIR</b>.
Use the squeue command to identify the job and job step of interest.
To initiate a checkpoint in which the job step will continue execution,
use the command: <br>
<b>scontrol check create <i>job_id.step_id</i></b><br>
To initiate a checkpoint in which the job step will terminate afterwards,
use the command: <br>
<b>scontrol check vacate <i>job_id.step_id</i></b></p>

<h3>Unsupported Options</h3>

<p>Some SLURM options can not be supported by PE and the following srun options
are silently ignored:</p>
<ul>
<li>-D, --chdir (set working directory)</li>
<li>-K, --kill-on-bad-exit (terminate step if any task has a non-zero exit code)</li>
<li>-k, --no-kill (set to not kill job upon node failure)</li>
<li>--ntasks-per-core (number of tasks to invoke per code)</li>
<li>--ntasks-per-socket (number of tasks to invoke per socket)</li>
<li>-O, --overcommit (over subscribe resources)</li>
<li>--resv-ports (communication ports reserved for OpenMPI)</li>
<li>--runjob-opts (used only on IBM BlueGene/Q systems)</li>
<li>--signal (signal to send when near time limit and the remaining time required)</li>
<li>--sockets-per-node (number of sockets per node required)</li>
<li>-u, --unbuffered (avoid line buffering)</li>
<li>-W, --wait (specify job swait time after first task exit)</li>
<li>-Z, --no-allocate (launch tasks without creating job allocation></li>
</ul>

<p>In addition, the following options have some limitations:</p>
<ul>
<li>-e, --error (Unable to support node and task-specific file names)</li>
<li>-i, --input (Unable to support node and task-specific file names)</li>
<li>-o, --output (Unable to support node and task-specific file names)</li>
</ul>

<h3>Environment Variables</h3>

<p><b>FIXME: We need to validate all of this</b>
<p>Since SLURM is not directly launching user tasks, the following environment
variables are NOT available with POE:</p>
<ul>
<li>SLURM_CPU_BIND_LIST</li>
<li>SLURM_CPU_BIND_TYPE</li>
<li>SLURM_CPU_BIND_VERBOSE</li>
<li>SLURM_CPUS_ON_NODE</li>
<li>SLURM_GTIDS</li>
<li>SLURM_LAUNCH_NODE_IPADDR</li>
<li>SLURM_LOCALID</li>
<li>SLURM_MEM_BIND_LIST</li>
<li>SLURM_MEM_BIND_TYPE</li>
<li>SLURM_MEM_BIND_VERBOSE</li>
<li>SLURM_PROCID</li>
<li>SLURM_TASK_PID</li>
</ul>

<h3>Gang Scheduling</h3>

<p>SLURM can be configured to gang schedule (time slice) parallel jobs by
alternately suspending and resuming them. Depending upon the the number of
jobs configured to time slice and the time slice interval (as specified in
the <i>slurm.conf</i> file using the <b>Shared</b> and <b>SchedulerTimeSlice</b>
options), the job may experience communication timeouts. Set the environment
variable <b>MP_TIMEOUT</b> to specify an appropriate communication timeout
value. Note that the default timeout is 150 seconds. See
<a href="gang_scheduling.html">Gang Scheduling</a> for more information.</p>
<pre>
export MP_TIMEOUT=600
</pre>

<h3>Other User Notes</h3>

<p>If the srun command's --label option is used, note that POE's prefix is
the task ID followed by a colon. The prefix with native SLURM is the task ID
followed by a colon followed by one space.</p>

<h2>System Administration</h2>

<p>There are several critical SLURM configuration parameters for use with PE.
These configuration parameters should be set in your <b>slurm.conf</b> file.
<b>LaunchType</b> defines the task launch mechanism to be used and must be set
to <b>launch/poe</b>.
<b>SwitchType</b> defines the mechanism used to manage the network switch and
it must be set to <b>switch/nrt</b> and use IBM's Network Resource Table (NRT)
interface.
Task launch is slower in this environment than with a typical Linux cluster
and the <b>MessageTimeout</b> must be configured to a sufficiently large value
so that large parallel jobs can be launched without SLURM's job step
credentials expiring.
When switch resources are allocated to a job, all processes spawned by that job
must be terminated before the switch resources can be released for use by another
program. This means that reliable tracking of all spawned processes is critical
for switch use. Use of <b>ProctrackType=proctrack/cgroup</b> is strongly
recommended. Use of any other process tracking plugin significantly increases
the likelyhood of orphan processes that must be manually identified and killed
in order to reuse switch resources.</p>
<pre>
# Excerpt of slurm.conf
LaunchType=launch/poe
SwitchType=switch/nrt
MessageTimeout=30
ProctrackType=proctrack/cgroup
</pre>

<p>In order for these plugins to be built, the locations of the POE Resource
Manager header file (permapi.h) the NRT header file (nrt.h) and NRT library
(libnrt.so) must be identified at the time the SLURM is built.
SLURM searches for the header files in the /usr/include directory by default.
If the files are not installed there, you can specify a different location using
the <b>--with-nrth=PATH</b> option to the configure program, where "PATH" is
the fully qualified pathname of the parent directory of the nrt.h and permapi.h
files.
SLURM searches for the libnrt.so file in the /usr/lib and /usr/lib64 directories
by default. If the file is not installed there, you can specify a different
location using the <b>--with-nrtlib=PATH</b> option to the configure program,
where "PATH" is the fully qualified pathname of the parent directory of the
libnrt.so file.
Alternately these values may be specified in your ~/.rpmmacros file.
For example:</p>
<pre>
%_with_nrth      "/opt/ibmhpc/pecurrent/base/include"
%_with_libnrt    "/opt/ibmhpc/pecurrent/base/intel/lib64"
</pre>

<p>The poe command interacts with SLURM by loading a SLURM library providing
a variety of functions for its use. You must specify the location of that
library and note that SLURM is the resource manager in the file named
"/etc/poe.limits".
The library name is "libpermapi.so" and it is in installed with the other SLURM
libraries in the subdirectory "lib/slurm". A sample "/etc/poe.limits" file is
shown below. You will need to modify the value of MP_PE_RMLIB to match SLURM's
installation location on your system.<br>
<b>NOTE:</b> The poe command is loading and using the libpermapi.so library
initially from the /usr/lib64 directory. It later reads the /etc/poe.limits
file and loads the correct library. In order for poe to work with SLURM it
needs to use the "libpermapi.so" generated by SLURM for all of its functions.
Until poe is modified to only load the correct library, it is necessary for
the file /usr/lib64/libpermapi.so to contain SLURM's library or a link to it.</p>
<pre>
#
# Sample /etc/poe.limits
# Modify the path below as appropriate
#
MP_PE_RMLIB=/usr/local/lib/slurm/libpermapi.so

</pre>

<h3>Job Scheduling</h3>

<p>SLURM can be configured to gang schedule (time slice) parallel jobs by
alternately suspending and resuming them. Depending upon the the number of
jobs configured to time slice and the time slice interval (as specified in
the <i>slurm.conf</i> file using the <b>Shared</b> and <b>SchedulerTimeSlice</b>
options), the job may experience communication timeouts. Set the environment
variable <b>MP_TIMEOUT</b> to specify an appropriate communication timeout
value. Note that the default timeout is 150 seconds. See
<a href="gang_scheduling.html">Gang Scheduling</a> for more information.</p>
<pre>
export MP_TIMEOUT=600
</pre>

<p>SLURM also can support long term job preemption with IBM's Parallel
Environment. Job's can be explicitly preempted and later resumed using the
<b>scontrol suspend &lt;jobid&gt;</b> and <b>scontrol resume &lt;jobid&gt;</b>
commands. This functionality relies upon NRT functions to suspend/resume
programs and reset MPI timeouts. Note that SLURM suports the preemption only
of whole jobs rather than individual job steps. A suspended job will relinquish
CPU resources, but retain memory and switch window resources. Note that the
long term suspension of jobs with any allocated Collective Acceleration
Units (CAU) is disabled and an error message to that effect will be generated
in response to such a request. In addition, verion 1200 or higher of IBM's NRT
API is required to support this functionality.</p>

<h3>Design Notes</h3>

<p>It is possible to configure SLURM and LoadLeveler to simultaneously exist
on a cluster, however each scheduler must be configured to manage different
compute nodes (e.g. LoadLeveler can manage compute nodes "tux[1-8]" and SLURM
can manaage compute nodes "tux[9-16]" on the same cluster).</p>

<p>The srun command uses the <b>launch/poe</b> plugin to launch the poe program.
Then poe uses the <b>launch/slurm</b> plugin to launch the "pmd" process on the
compute nodes, so two launch plugins are actually used.</p>

<p>Depending upon job size and network options, allocating and deallocating
switch resources can take multple seconds per node and the process of launching
applications on multiple nodes is not well parallelized.
This is outside of SLURM's control.</p>

<h3>Debugging Notes</h3>

<p>It is possible to generate detailed logging of all switch/nrt actions and
data by configuring <b>DebugFlags=switch</b>.</p>

<p>The Protocol Network Services Daemon (PNSD) manages the Network Resource
Table (NRT) information on each node. It's logs are written to the file
<b>/etc/serverlog</b>, which may be useful to diagnose problems. In order to
execute PNSD in debug node (for extra debugging information), run the following
commands as user root:</p>
<pre>
stopsrc -s pnsd
startsrc -s pnsd -a -D
</pre>

<p class="footer"><a href="#top">top</a></p>

<p style="text-align:center;">Last modified 13 August 2012</p></td>

<!--#include virtual="footer.txt"-->
