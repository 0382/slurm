<!--#include virtual="header.txt"-->

<h1>Generic Resource (GRES) Scheduling</h1>

<h2>Contents</h2>
<a href="#Overview">Overview</a><br>
<a href="#Configuration">Configuration</a><br>
<a href="#Running_Jobs">Running Jobs</a><br>
<a href="#GPU_Management">GPU Management</a><br>
<a href="#MPS_Management">MPS Management</a><br>
<a href="#MIC_Management">MIC Management</a><br>

<!-------------------------------------------------------------------------->
<a id="Overview"></a>
<h2>Overview</h2>
<P>Generic resource (GRES) scheduling is supported through a flexible plugin
mechanism. Support is currently provided for Graphics Processing Units (GPUs),
CUDA Multi-Process Service (MPS), and
Intel&reg; Many Integrated Core (MIC) processors.</P>

<!-------------------------------------------------------------------------->
<a id="Configuration"></a>
<h2>Configuration</h2>

<P>Slurm supports no generic resources in the default configuration.
One must explicitly specify which resources are to be managed in the
<I>slurm.conf</I> configuration file. The configuration parameters of
interest are:</P>

<UL>
<LI><B>GresTypes</B> a comma delimited list of generic resources to be
managed (e.g. <I>GresTypes=gpu,mps</I>). This name may be that of an
optional plugin providing additional control over the resources.</LI>
<LI><B>Gres</B> the generic resource configuration details in the format<br>
&lt;name&gt;[:&lt;type&gt;][:no_consume]:&lt;number&gt;[K|M|G]<br>
The first field is the resource name, which matches the GresType configuration
parameter name.
The optional type field might be used to identify a model of that generic
resource.
A generic resource can also be specified as non-consumable (i.e. multiple
jobs can use the same generic resource) with the optional field ":no_consume".
The final field must specify a generic resource count.
A suffix of "K", "M" or "G" may be used to multiply the count by 1024,
1048576 or 1073741824 respectively.
By default a node has no generic resources.</LI>
</UL>

<P>Note that the GRES specification for each node works in the same fashion
as the other resources managed. Depending upon the value of the
<I>FastSchedule</I> parameter, nodes which are found to have fewer resources
than configured will be placed in a DOWN state.</P>

<P>Sample slurm.conf file:</P>
<PRE>
# Configure support for our four GPUs
GresTypes=gpu,mps,bandwidth
NodeName=tux[0-7] Gres=gpu:tesla:2,gpu:kepler:2,mps:400,bandwidth:lustre:no_consume:4G
</PRE>

<P>Each compute node with generic resources typically contain a <I>gres.conf</I>
file describing which resources are available on the node, their count, 
associated device files and cores which should be used with those resources.
In the case of GPUs, their configuration will be automatically gathered using
the NVML library, if installed.
Configuration information about all other generic resource must explicitly be
described in the <I>gres.conf</I> file.
The configuration parameters available are:</P>

<UL>
<LI><B>Name</B> name of a generic resource (must match <B>GresTypes</B>
values in <I>slurm.conf</I>).</LI>

<LI><B>Count</B> Number of resources of this type available on this node.
The default value is set to the number of <B>File</B> values specified (if any),
otherwise the default value is one. A suffix of "K", "M" or "G" may be used
to multiply the number by 1024, 1048576 or 1073741824 respectively
(e.g. "Count=10G"). Note that Count is a 32-bit field and the maximum value
is 4,294,967,295.</LI>

<LI><B>Cores</B>
Specify the first thread CPU index numbers for the specific cores which can
use this resource.
For example, it may be strongly preferable
to use specific cores with specific devices (e.g. on a NUMA
architecture). Multiple cores may be specified using a comma
delimited list or a range may be specified using a "-" separator
(e.g. "0,1,2,3" or "0-3").
<B>If specified, then only the identified cores can be allocated with each generic
resource; an attempt to use other cores will not be honored.</B>
If not specified, then any core can be used with the resources, which also
increases the speed of Slurm's scheduling algorithm.
If any core can be effectively used with the resources, then do not specify the
Cores option for improved speed in the Slurm scheduling logic.

<B>NOTE:</B> If your cores contain multiple threads only list the first thread
of each core. The logic is such that it uses core instead of thread scheduling
per GRES. Also note that since Slurm must be able to perform resource
management on heterogeneous clusters having various core ID numbering schemes,
an abstract index will be used instead of the physical core index. That
abstract id may not correspond to your physical core number.
Basically Slurm starts numbering from 0 to n, being 0 the id of the first
processing unit (core or thread if HT is enabled) on the first socket,
first core and maybe first thread, and then continuing sequentially to the
next thread, core, and socket. The numbering generally coincides with the
processing unit logical number (PU L#) seen in lstopo output.

<LI><B>File</B> Fully qualified pathname of the device files associated with a
resource.
The name can include a numeric range suffix to be interpreted by Slurm
(e.g. <I>File=/dev/nvidia[0-3]</I>).
This field is generally required if enforcement of generic resource
allocations are to be supported (i.e. prevents a user from making
use of resources allocated to a different user).
Enforcement of the file allocation relies upon Linux Control Groups (cgroups)
and Slurm's task/cgroup plugin, which will place the allocated files into
the job's cgroup and prevent use of other files.
Please see Slurm's <a href="cgroups.html">Cgroups Guide</a> for more
information.<br>
Except in the case of MPS support, if <B>File</B> is specified then <B>Count</B>
must be either set to the number of file names specified or not set (the
default value is the number of files specified).
In the case of MPS support, each GPU would be identifed by name using the
<B>File</B> parameter and <B>Count</B> would specify the number of MPS entries
that would correspond to that GPU; typically 100 or some multiple of 100.
<br>
NOTE: If you specify the <B>File</B> parameter for a resource on some node,
the option must be specified on all nodes and Slurm will track the assignment
of each specific resource on each node. Otherwise Slurm will only track a
count of allocated resources rather than the state of each individual device
file.
<br>
NOTE: Drain a node before changing the count of records with <B>File</B>
parameters (i.e. if you want to add or remove GPUs from a node's configuration).
Failure to do so will result in any job using those GRES being aborted.</LI>

<LI><B>Type</B> Optionally specify the device type. For example, this might
be used to identify a specific model of GPU, which users can then specify
in their job request.
If <B>Type</B> is specified, then <B>Count</B> is limited in size (currently 1024).
</LI>
</UL>

<P>Sample gres.conf file:</P>
<PRE>
# Configure support for four GPUs (with MPS), plus bandwidth
Name=gpu Type=tesla  File=/dev/nvidia0 Cores=0,1
Name=gpu Type=tesla  File=/dev/nvidia1 Cores=0,1
Name=gpu Type=kepler File=/dev/nvidia2 Cores=2,3
Name=gpu Type=kepler File=/dev/nvidia3 Cores=2,3
Name=mps Count=100  File=/dev/nvidia0
Name=mps Count=100  File=/dev/nvidia1
Name=mps Count=100  File=/dev/nvidia2
Name=mps Count=100  File=/dev/nvidia3
Name=bandwidth Type=lustre Count=4G
</PRE>
<!-------------------------------------------------------------------------->
<a id="Running_Jobs"></a>
<h2>Running Jobs</h2>

<P>Jobs will not be allocated any generic resources unless specifically
requested at job submit time using the options:</P>
<DL>
<DT><I>--gres</I></DT>
<DD>Generic resources required per node</DD>
<DT><I>--gpu</I></DT>
<DD>GPUs required per job</DD>
<DT><I>--gpu-per-node</I></DT>
<DD>GPUs required per node. Equivalent to the <I>--gres</I> option for GPUs.</DD>
<DT><I>--gpu-per-socket</I></DT>
<DD>GPUs required per socket. Requires the job to specify a task socket.</DD>
<DT><I>--gpu-per-task</I></DT>
<DD>GPUs required per task. Requires the job to specify a task count.</DD>
</DL>

<P>All of these options are supported by the <I>salloc</I>, <I>sbatch</I> and
<I>srun</I> commands.
Note that all of the <I>--gpu*</I> options are only supported by Slurm's
select/cons_tres plugin.
Jobs requesting these options when the select/cons_tres plugin is <U>not</U>
configured will be rejected.
The <I>--gres</I> option requires an argument specifying which generic resources
are required and how many resources using the form <I>name[:type:count]</I>
while all of the <I>--gpu*</I> options require an argument of the form
 <I>[type]:count</I>.
The <I>name</I> is the same name as
specified by the <I>GresTypes</I> and <I>Gres</I> configuration parameters.
<I>type</I> identifies a specific type of that generic resource (e.g. a
specific model of GPU).
<I>count</I> specifies how many resources are required and has a default
value of 1. For example:<BR> 
<I>sbatch --gres=gpu:kepler:2 ...</I>.</P>

<P>Several addition resource requirement specifications are available
specifically for GPUs and detailed descriptions about these options are
available in the man pages for the job submission commands.
As for the <I>--gpu*</I> option, these options are only supported by Slurm's
select/cons_tres plugin.</P>
</P>
<DL>
<DT><I>--cpus-per-gpu</I></DT>
<DD>Count of CPUs allocated per GPU.</DD>
<DT><I>--gpu-bind</I></DT>
<DD>Define how tasks are bound to GPUs.</DD>
<DT><I>--gpu-freq</I></DT>
<DD>Specify GPU frequency and/or GPU memory frequency.</DD>
<DT><I>--mem-per-gpu</I></DT>
<DD>Memory allocated per GPU.</DD>
</DL>

<P>Jobs will be allocated specific generic resources as needed to satisfy
the request. If the job is suspended, those resources do not become available
for use by other jobs.</P>

<P>Job steps can be allocated generic resources from those allocated to the
job using the <I>--gres</I> option with the <I>srun</I> command as described
above. By default, a job step will be allocated all of the generic resources
allocated to the job. If desired, the job step may explicitly specify a
different generic resource count than the job.
This design choice was based upon a scenario where each job executes many
job steps. If job steps were granted access to all generic resources by
default, some job steps would need to explicitly specify zero generic resource
counts, which we considered more confusing. The job step can be allocated
specific generic resources and those resources will not be available to other
job steps. A simple example is shown below.</P>

<PRE>
#!/bin/bash
#
# gres_test.bash
# Submit as follows:
# sbatch --gres=gpu:4 -n4 -N1-1 gres_test.bash
#
srun --gres=gpu:2 -n2 --exclusive show_device.sh &
srun --gres=gpu:1 -n1 --exclusive show_device.sh &
srun --gres=gpu:1 -n1 --exclusive show_device.sh &
wait
</PRE>

<!-------------------------------------------------------------------------->
<a id="GPU_Management"></a>
<h2>GPU Management</h2>

<P>In the case of Slurm's GRES plugin for GPUs, the environment variable
<code class="commandline">CUDA_VISIBLE_DEVICES</code>
is set for each job step to determine which GPUs are
available for its use on each node. This environment variable is only set
when tasks are launched on a specific compute node (no global environment
variable is set for the <i>salloc</i> command and the environment variable set
for the <i>sbatch</i> command only reflects the GPUs allocated to that job
on that node, node zero of the allocation).
CUDA version 3.1 (or higher) uses this environment
variable in order to run multiple jobs or job steps on a node with GPUs
and ensure that the resources assigned to each are unique. In the example
above, the allocated node may have four or more graphics devices. In that
case, <code class="commandline">CUDA_VISIBLE_DEVICES</code>
will reference unique devices for each file and
the output might resemble this:</P>

<PRE>
JobStep=1234.0 CUDA_VISIBLE_DEVICES=0,1
JobStep=1234.1 CUDA_VISIBLE_DEVICES=2
JobStep=1234.2 CUDA_VISIBLE_DEVICES=3
</PRE>

<P>NOTE: Be sure to specify the <I>File</I> parameters in the <I>gres.conf</I>
file and ensure they are in the increasing numeric order.</P>

<p>When possible, Slurm automatically determines the GPUs on the system using
NVIDIA's NVML library. The NVML library (which powers the
<code class="commandline">nvidia-smi</code> tool) numbers GPUs in order by their
PCI bus IDs. For this numbering to match the numbering reported by CUDA, the
<code class="commandline">CUDA_DEVICE_ORDER</code> environmental variable must
be set to <code class="commandline">CUDA_DEVICE_ORDER=PCI_BUS_ID</code>.</p>

<p>GPU device files (e.g. <i>/dev/nvidia1</i>) are
based on the Linux minor number assignment, while NVML's device numbers are
assigned via PCI bus ID, from lowest to highest. Mapping between these two is
indeterministic and system dependent, and could vary between boots after
hardware or OS changes. For the most part, this assignment seems fairly stable.
However, an after-bootup check is required to guarantee that a GPU device is
assigned to a specific device file.</p>

<p>Please consult the
<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars">
NVIDIA CUDA documentation</a> for more information about the
<code class="commandline">CUDA_VISIBLE_DEVICES</code> and
<code class="commandline">CUDA_DEVICE_ORDER</code> environmental variables.</p>

<!-------------------------------------------------------------------------->
<a id="MPS_Management"></a>
<h2>MPS Management</h2>

<p>CUDA Multi-Process Service (MPS) provides a mechanism where GPUs can be
shared by multiple jobs, where each job is allocated some percentage of the
GPU's resources.
GPUs to be made available for MPS should be identified in the <I>slurm.conf</I>
and <I>gres.conf</I> configuration files with each device <B>File</B> being
explicitly named and its <B>Count</B> specified (typically a multiple of 100).
Alternately, specify <B>Count</B> without <B>File</B> and the value of
<B>Count</B> value will be distributed evenly across all defined GPUs.
If the GPU sharing that <B>File</B> name contains <B>Core</B> or <B>Type</B>
parameters, that information will be automatically copied to the corresponding
MPS GRES.
Note that the same GPU can be allocated either as a GPU type GRES or as an
MPS type GRES, but not both.
In other words, once a GPU has been allocated as a gres/gpu resource it will
not be available as a gres/mps.
Likewise, once a GPU has been allocated as a gres/mps resource it will
not be available as a gres/gpu.</p>

<P><B>NOTE:</B> Slurm support for gres/mps requires the use of the
select/cons_tres plugin.</P>

<p>Jobs requesting MPS resources will have the same environment variables set
as for GPU resources, namely <code class="commandline">CUDA_VISIBLE_DEVICES</code>
and <code class="commandline">CUDA_DEVICE_ORDER</code>.
In addition they will have the
<code class="commandline">CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code>
environment variable set where its value is the job's percentage of MPS
resources available on the assigned GPU.</p>

<p>Please consult the
<a href="https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf">
NVIDIA Multi-Process Service documentation</a> for more information about MPS.</p>

<!-------------------------------------------------------------------------->
<a id="MIC_Management"></a>
<h2>MIC Management</h2>

<P>Slurm can be used to provide resource management for systems with the
Intel&reg; Many Integrated Core (MIC) processor.
Slurm sets an <code class="commandline">OFFLOAD_DEVICES</code>
environment variable, which controls the
selection of MICs available to a job step.
The <code class="commandline">OFFLOAD_DEVICES</code>
environment variable is used by both Intel
LEO (Language Extensions for Offload) and the MKL (Math Kernel Library)
automatic offload.
(This is very similar to how the
<code class="commandline">CUDA_VISIBLE_DEVICES</code> environment variable is
used to control which GPUs can be used by CUDA&trade; software.)
If no MICs are reserved via GRES, the
<code class="commandline">OFFLOAD_DEVICES</code> variable is set to
-1. This causes the code to ignore the offload directives and run MKL
routines on the CPU. The code will still run but only on the CPU. This
also gives a somewhat cryptic warning:</P>
<pre>offload warning: OFFLOAD_DEVICES device number -1 does not correspond
to a physical device</pre>
<P>The offloading is automatically scaled to all the devices, (e.g. if
--gres=mic:2 is defined) then all offloads use two MICs unless
explicitly defined in the offload pragmas.</P>
<!-------------------------------------------------------------------------->

<p style="text-align: center;">Last modified 31 December 2018</p>

<!--#include virtual="footer.txt"-->
